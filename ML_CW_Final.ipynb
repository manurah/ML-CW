{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd8XdhuQRA8P"
      },
      "source": [
        "#**Task 01**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "on1U0SnVaISs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CihLRpWlRA8R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Create a directory to save plots for your report\n",
        "OUTPUT_DIR = \"eda_plots\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Plots will be saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ydata-profiling"
      ],
      "metadata": {
        "id": "0S0khc-LOK80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKg5b9qXRA8R"
      },
      "source": [
        "### 1. Load Dataset (Local File)\n",
        "Make sure the filename below matches the file you uploaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IWAPcxURA8S"
      },
      "outputs": [],
      "source": [
        "# Define the filename (Change this if your file has a different name)\n",
        "filename = '/content/drive/MyDrive/cw/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "# Fallback name often used\n",
        "filename_alt = 'Telco-Customer-Churn.csv'\n",
        "\n",
        "if os.path.exists(filename):\n",
        "    file_path = filename\n",
        "elif os.path.exists(filename_alt):\n",
        "    file_path = filename_alt\n",
        "else:\n",
        "    file_path = None\n",
        "    print(\"ERROR: CSV file not found! Please upload your dataset to the Colab Files tab on the left.\")\n",
        "\n",
        "if file_path:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Dataset loaded successfully from {file_path}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    display(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ydata_profiling import ProfileReport\n",
        "\n",
        "profile = ProfileReport(df, title=\"Profiling Report\")\n",
        "\n",
        "profile.to_notebook_iframe()"
      ],
      "metadata": {
        "id": "DnUwDOBIOmIf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile.to_file(\"report.html\")"
      ],
      "metadata": {
        "id": "AdXbR_kTQG43"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cRwjmpBRA8S"
      },
      "source": [
        "### 2. Data Cleaning & Preprocessing\n",
        "We need to fix `TotalCharges` (convert to numeric) and handle `SeniorCitizen` encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAoaTpx5RA8S"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    # 1. 'TotalCharges' is often read as object because of empty strings \" \" for new customers.\n",
        "    # We force it to numeric, converting errors to NaN\n",
        "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "    # 2. Fill missing TotalCharges with 0 (these are usually new customers with 0 tenure)\n",
        "    null_count = df['TotalCharges'].isnull().sum()\n",
        "    print(f\"Found {null_count} missing values in TotalCharges. Filling with 0.\")\n",
        "    df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
        "\n",
        "    # 3. SeniorCitizen is 0/1, map to Yes/No for better graphs\n",
        "    df['SeniorCitizen'] = df['SeniorCitizen'].replace({1: 'Yes', 0: 'No'})\n",
        "\n",
        "    # 4. Drop customerID as it is not useful for analysis\n",
        "    if 'customerID' in df.columns:\n",
        "        df = df.drop('customerID', axis=1)\n",
        "\n",
        "    print(\"Data Cleaning Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QJaPBMRRA8S"
      },
      "source": [
        "### 3. Target Distribution (Churn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCvIcpt2RA8S"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    ax = sns.countplot(x='Churn', data=df, palette='viridis')\n",
        "    plt.title('Distribution of Churn (Target Variable)')\n",
        "    plt.xlabel('Churn Status')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Add percentages\n",
        "    total = len(df)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        if height > 0:\n",
        "            percentage = '{:.1f}%'.format(100 * height/total)\n",
        "            x = p.get_x() + p.get_width() / 2 - 0.05\n",
        "            y = height\n",
        "            ax.annotate(percentage, (x, y), ha='center', va='bottom')\n",
        "\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/1_churn_distribution.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NpmV2GDRA8T"
      },
      "source": [
        "### 4. Numerical Features Analysis\n",
        "Checking Tenure, Monthly Charges, and Total Charges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiNd-PIaRA8T"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(num_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.histplot(data=df, x=col, hue='Churn', kde=True, element=\"step\", palette='viridis')\n",
        "        plt.title(f'{col} Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/2_numerical_distributions.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNMIOsm2RA8T"
      },
      "source": [
        "### 4.5. Outlier Detection (Boxplots)\n",
        "Identifying extreme values in numerical data is crucial for understanding data quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8O8KllkRA8T"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(num_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.boxplot(x=df[col], palette='viridis')\n",
        "        plt.title(f'{col} Boxplot')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/2b_outlier_detection.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgZe6odvRA8T"
      },
      "source": [
        "### 5. Categorical Features Analysis\n",
        "Checking Contracts, Payment Methods, Internet Service, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrEgVSWdRA8T"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    cat_cols = ['Gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
        "                'PhoneService', 'InternetService', 'Contract', 'PaymentMethod']\n",
        "\n",
        "    # Filter cols that actually exist\n",
        "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
        "\n",
        "    fig, axes = plt.subplots(4, 2, figsize=(16, 20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(cat_cols):\n",
        "        if i < len(axes):\n",
        "            sns.countplot(data=df, x=col, hue='Churn', ax=axes[i], palette='viridis')\n",
        "            axes[i].set_title(f'Churn Rate by {col}')\n",
        "            axes[i].tick_params(axis='x', rotation=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/3_categorical_features.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm2lqP5URA8U"
      },
      "source": [
        "### 6. Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJhS2v2GRA8U"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    # Convert Churn to numeric for correlation\n",
        "    df_corr = df.copy()\n",
        "    df_corr['Churn'] = df_corr['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "    # Select only numeric columns\n",
        "    numeric_df = df_corr.select_dtypes(include=[np.number])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/4_correlation_heatmap.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Task 02**"
      ],
      "metadata": {
        "id": "wv9J2wnsRXc8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1. Setup"
      ],
      "metadata": {
        "id": "eY0wptNDc9gV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Scikit-Learn Modules\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, roc_curve, auc, f1_score)\n",
        "\n",
        "# --- 1. SETUP & CONFIGURATION ---\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Visualization settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create output folder\n",
        "OUTPUT_DIRE = \"task02_results\"\n",
        "os.makedirs(OUTPUT_DIRE, exist_ok=True)\n",
        "print(f\"Output Directory Created: {OUTPUT_DIRE}/\")"
      ],
      "metadata": {
        "id": "GJK2__TTRVDK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2. Data loading & Cleaning"
      ],
      "metadata": {
        "id": "bHFsN_sicVuQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[Step 2] Loading Dataset...\")\n",
        "\n",
        "filename = '/content/drive/MyDrive/cw/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "if os.path.exists(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    print(\" -> Loaded local file.\")\n",
        "else:\n",
        "    print(\"ERROR: CSV file not found! Please upload your dataset to the Colab Files tab on the left.\")\n",
        "\n",
        "# Data Cleaning\n",
        "# Fix TotalCharges: It has spaces \" \" for new customers. Coerce creates NaNs, we fill with 0.\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)\n",
        "\n",
        "# Drop customerID (irrelevant unique identifier)\n",
        "if 'customerID' in df.columns:\n",
        "    df = df.drop(['customerID'], axis=1)\n",
        "\n",
        "# Encode Target: Yes -> 1, No -> 0\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "print(f\" -> Data Shape after cleaning: {df.shape}\")"
      ],
      "metadata": {
        "id": "idGsIs0ucwg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3. Preprocessing"
      ],
      "metadata": {
        "id": "VckYOMBfdLp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[Step 3] Preprocessing & Feature Engineering...\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "# We use drop_first=True to avoid multicollinearity (dummy variable trap)\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "X = df_encoded.drop('Churn', axis=1)\n",
        "y = df_encoded['Churn']\n",
        "\n",
        "# Stratified Split\n",
        "# 'stratify=y' ensures the % of churners is the same in Train and Test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "# Neural Networks perform significantly better with scaled data (Mean=0, Std=1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\" -> Data successfully split and scaled.\")"
      ],
      "metadata": {
        "id": "kcad96ysdcRX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4. Decision Tree"
      ],
      "metadata": {
        "id": "UdTPvTunden5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[Step 4] Training Decision Tree...\")\n",
        "\n",
        "# Parameter Grid\n",
        "# Included 'class_weight' to handle imbalance automatically\n",
        "dt_params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 10, 20],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "dt_grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
        "    dt_params,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "dt_grid.fit(X_train_scaled, y_train)\n",
        "best_dt = dt_grid.best_estimator_\n",
        "\n",
        "print(f\"\\n -> Best Tree Params: {dt_grid.best_params_}\")"
      ],
      "metadata": {
        "id": "JiAihSO7dmoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5. Neural Network"
      ],
      "metadata": {
        "id": "C0Z_N5xRdvr8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[Step 5] Training Neural Network...\")\n",
        "print(\" -> Tuning Hyperparameters (this takes a moment)...\")\n",
        "\n",
        "# Parameter Grid\n",
        "nn_params = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'alpha': [0.0001, 0.01], # L2 regularization\n",
        "    'learning_rate_init': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "# MLP Classifier\n",
        "mlp = MLPClassifier(max_iter=500, random_state=RANDOM_SEED)\n",
        "\n",
        "# Grid Search\n",
        "nn_grid = GridSearchCV(\n",
        "    mlp,\n",
        "    nn_params,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "nn_grid.fit(X_train_scaled, y_train)\n",
        "best_nn = nn_grid.best_estimator_\n",
        "\n",
        "print(f\" -> Best Neural Net Params: {nn_grid.best_params_}\")"
      ],
      "metadata": {
        "id": "h14JPhV1d4j3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###6. Generating Plots"
      ],
      "metadata": {
        "id": "H6OINRE0d-fx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n[Step 6] Evaluation & Generating Plots...\")\n",
        "\n",
        "# Function to plot Confusion Matrix\n",
        "def plot_cm(model, X_test, y_test, title, filename):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Actual Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIRE, filename), dpi=300)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MJBQVj_EeHI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###7. Evaluation"
      ],
      "metadata": {
        "id": "_eZ7jAgsIySR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Decision Tree Evaluation\n",
        "print(\"\\n--- Decision Tree Results ---\")\n",
        "y_pred_dt = best_dt.predict(X_test_scaled)\n",
        "dt_acc = accuracy_score(y_test, y_pred_dt)\n",
        "print(f\"Decision Tree Accuracy: {dt_acc:.4f}\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "# Plot Confusion Matrix for DT\n",
        "plot_cm(best_dt, X_test_scaled, y_test, \"Decision Tree Confusion Matrix\", \"cm_dt.png\")\n",
        "\n",
        "\n",
        "# 2. Neural Network Evaluation\n",
        "print(\"\\n--- Neural Network Results ---\")\n",
        "y_pred_nn = best_nn.predict(X_test_scaled)\n",
        "nn_acc = accuracy_score(y_test, y_pred_nn)\n",
        "print(f\"Neural Network Accuracy: {nn_acc:.4f}\")\n",
        "print(classification_report(y_test, y_pred_nn))\n",
        "\n",
        "# Plot Confusion Matrix for NN\n",
        "plot_cm(best_nn, X_test_scaled, y_test, \"Neural Network Confusion Matrix\", \"cm_nn.png\")"
      ],
      "metadata": {
        "id": "iJZhlRMTI4_Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Experiment - Applying SMOTE (Oversampling)"
      ],
      "metadata": {
        "id": "ekN-BB2SQtyY"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "044f0980"
      },
      "source": [
        "get_ipython().system('pip install imblearn')\n",
        "print(\"imblearn installed.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c928e6b"
      },
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "print(\"\\nApplying SMOTE for class balancing...\")\n",
        "\n",
        "smote = SMOTE(random_state=RANDOM_SEED)\n",
        "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "print(f\" -> Original training set shape: {X_train_scaled.shape}, {y_train.shape}\")\n",
        "print(f\" -> SMOTE balanced training set shape: {X_train_smote.shape}, {y_train_smote.shape}\")\n",
        "print(f\" -> Class distribution after SMOTE: {y_train_smote.value_counts()}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7ab8405"
      },
      "source": [
        "print(\"\\nRetraining Decision Tree with SMOTE balanced data...\")\n",
        "\n",
        "# Re-initialize GridSearchCV for Decision Tree with SMOTE data\n",
        "dt_grid_smote = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
        "    dt_params,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "dt_grid_smote.fit(X_train_smote, y_train_smote)\n",
        "best_dt_smote = dt_grid_smote.best_estimator_\n",
        "\n",
        "print(f\" -> Best Tree Params (SMOTE): {dt_grid_smote.best_params_}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ef45bc5"
      },
      "source": [
        "print(\n",
        "    \"\\nRetraining Neural Network with SMOTE balanced data...\"\n",
        ")\n",
        "print(\" -> Tuning Hyperparameters (this takes a moment)...\")\n",
        "\n",
        "# Re-initialize GridSearchCV for Neural Network with SMOTE data\n",
        "# Use the same parameters as before\n",
        "nn_grid_smote = GridSearchCV(\n",
        "    MLPClassifier(max_iter=500, random_state=RANDOM_SEED),\n",
        "    nn_params,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1,\n",
        ")\n",
        "\n",
        "nn_grid_smote.fit(X_train_smote, y_train_smote)\n",
        "best_nn_smote = nn_grid_smote.best_estimator_\n",
        "\n",
        "print(f\" -> Best Neural Net Params (SMOTE): {nn_grid_smote.best_params_}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cf671525"
      },
      "source": [
        "print(\"\\n--- Decision Tree Results (SMOTE) ---\")\n",
        "y_pred_dt_smote = best_dt_smote.predict(X_test_scaled)\n",
        "dt_acc_smote = accuracy_score(y_test, y_pred_dt_smote)\n",
        "print(f\"Decision Tree Accuracy (SMOTE): {dt_acc_smote:.4f}\")\n",
        "print(classification_report(y_test, y_pred_dt_smote))\n",
        "\n",
        "# Plot Confusion Matrix for DT (SMOTE)\n",
        "plot_cm(best_dt_smote, X_test_scaled, y_test, \"Decision Tree Confusion Matrix (SMOTE)\", \"cm_dt_smote.png\")\n",
        "\n",
        "\n",
        "print(\"\\n--- Neural Network Results (SMOTE) ---\")\n",
        "y_pred_nn_smote = best_nn_smote.predict(X_test_scaled)\n",
        "nn_acc_smote = accuracy_score(y_test, y_pred_nn_smote)\n",
        "print(f\"Neural Network Accuracy (SMOTE): {nn_acc_smote:.4f}\")\n",
        "print(classification_report(y_test, y_pred_nn_smote))\n",
        "\n",
        "# Plot Confusion Matrix for NN (SMOTE)\n",
        "plot_cm(best_nn_smote, X_test_scaled, y_test, \"Neural Network Confusion Matrix (SMOTE)\", \"cm_nn_smote.png\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e55c113c"
      },
      "source": [
        "### Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **SMOTE Application:** The SMOTE oversampling technique successfully balanced the training data, increasing the minority class ('Churn 1') to match the majority class ('Churn 0'). The original training set contained 5634 samples, which expanded to 8278 samples after SMOTE, with an equal distribution of 4139 samples for each class.\n",
        "*   **Decision Tree Model Retraining & Evaluation:**\n",
        "    *   The Decision Tree model, retrained with SMOTE-balanced data, achieved an accuracy of 0.7317 on the test set.\n",
        "    *   For the minority class (Class 1), it showed a Precision of 0.50, Recall of 0.66, and an F1-score of 0.57. The best hyperparameters found were `{'class_weight': 'balanced', 'criterion': 'gini', 'max_depth': 10, 'min_samples_split': 2}`.\n",
        "*   **Neural Network Model Retraining & Evaluation:**\n",
        "    *   The Neural Network model, retrained with SMOTE-balanced data, achieved an accuracy of 0.7530 on the test set.\n",
        "    *   For the minority class (Class 1), it exhibited a Precision of 0.53, Recall of 0.59, and an F1-score of 0.56. The best hyperparameters were `{'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,), 'learning_rate_init': 0.001}`.\n",
        "    *   A `ConvergenceWarning` was noted during Neural Network training, suggesting the model did not fully converge.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Both models showed improved handling of the minority class (Class 1) compared to likely imbalanced training, with Recall values of 0.66 for Decision Tree and 0.59 for Neural Network, which are crucial for identifying churn. The Neural Network achieved slightly higher overall accuracy (0.7530 vs. 0.7317) and F1-score for the minority class (0.56 vs. 0.57).\n",
        "*   Further investigation into the Neural Network's `ConvergenceWarning` is recommended. Techniques like increasing the maximum number of iterations, adjusting the learning rate, or using different optimizers could potentially improve its performance and ensure full convergence.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
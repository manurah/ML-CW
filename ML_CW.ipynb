{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bd8XdhuQRA8P"
      },
      "source": [
        "**Task 01**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "on1U0SnVaISs"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[0;32m      2\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'google'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CihLRpWlRA8R"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Set visualization style\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "# Create a directory to save plots for your report\n",
        "OUTPUT_DIR = \"eda_plots\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Plots will be saved to: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKg5b9qXRA8R"
      },
      "source": [
        "### 1. Load Dataset (Local File)\n",
        "Make sure the filename below matches the file you uploaded."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8IWAPcxURA8S"
      },
      "outputs": [],
      "source": [
        "# Define the filename (Change this if your file has a different name)\n",
        "filename = '/content/drive/MyDrive/cw/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "# Fallback name often used\n",
        "filename_alt = 'Telco-Customer-Churn.csv'\n",
        "\n",
        "if os.path.exists(filename):\n",
        "    file_path = filename\n",
        "elif os.path.exists(filename_alt):\n",
        "    file_path = filename_alt\n",
        "else:\n",
        "    file_path = None\n",
        "    print(\"ERROR: CSV file not found! Please upload your dataset to the Colab Files tab on the left.\")\n",
        "\n",
        "if file_path:\n",
        "    df = pd.read_csv(file_path)\n",
        "    print(f\"Dataset loaded successfully from {file_path}\")\n",
        "    print(f\"Shape: {df.shape}\")\n",
        "    display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cRwjmpBRA8S"
      },
      "source": [
        "### 2. Data Cleaning & Preprocessing\n",
        "We need to fix `TotalCharges` (convert to numeric) and handle `SeniorCitizen` encoding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xAoaTpx5RA8S"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    # 1. 'TotalCharges' is often read as object because of empty strings \" \" for new customers.\n",
        "    # We force it to numeric, converting errors to NaN\n",
        "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
        "\n",
        "    # 2. Fill missing TotalCharges with 0 (these are usually new customers with 0 tenure)\n",
        "    null_count = df['TotalCharges'].isnull().sum()\n",
        "    print(f\"Found {null_count} missing values in TotalCharges. Filling with 0.\")\n",
        "    df['TotalCharges'] = df['TotalCharges'].fillna(0)\n",
        "\n",
        "    # 3. SeniorCitizen is 0/1, map to Yes/No for better graphs\n",
        "    df['SeniorCitizen'] = df['SeniorCitizen'].replace({1: 'Yes', 0: 'No'})\n",
        "\n",
        "    # 4. Drop customerID as it is not useful for analysis\n",
        "    if 'customerID' in df.columns:\n",
        "        df = df.drop('customerID', axis=1)\n",
        "\n",
        "    print(\"Data Cleaning Complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QJaPBMRRA8S"
      },
      "source": [
        "### 3. Target Distribution (Churn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCvIcpt2RA8S"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    ax = sns.countplot(x='Churn', data=df, palette='viridis')\n",
        "    plt.title('Distribution of Churn (Target Variable)')\n",
        "    plt.xlabel('Churn Status')\n",
        "    plt.ylabel('Count')\n",
        "\n",
        "    # Add percentages\n",
        "    total = len(df)\n",
        "    for p in ax.patches:\n",
        "        height = p.get_height()\n",
        "        if height > 0:\n",
        "            percentage = '{:.1f}%'.format(100 * height/total)\n",
        "            x = p.get_x() + p.get_width() / 2 - 0.05\n",
        "            y = height\n",
        "            ax.annotate(percentage, (x, y), ha='center', va='bottom')\n",
        "\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/1_churn_distribution.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5NpmV2GDRA8T"
      },
      "source": [
        "### 4. Numerical Features Analysis\n",
        "Checking Tenure, Monthly Charges, and Total Charges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiNd-PIaRA8T"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(num_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.histplot(data=df, x=col, hue='Churn', kde=True, element=\"step\", palette='viridis')\n",
        "        plt.title(f'{col} Distribution')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/2_numerical_distributions.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNMIOsm2RA8T"
      },
      "source": [
        "### 5. Outlier Detection (Boxplots)\n",
        "Identifying extreme values in numerical data is crucial for understanding data quality."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8O8KllkRA8T"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    num_cols = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
        "\n",
        "    plt.figure(figsize=(15, 5))\n",
        "    for i, col in enumerate(num_cols):\n",
        "        plt.subplot(1, 3, i+1)\n",
        "        sns.boxplot(x=df[col], palette='viridis')\n",
        "        plt.title(f'{col} Boxplot')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/2b_outlier_detection.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tgZe6odvRA8T"
      },
      "source": [
        "### 6. Categorical Features Analysis\n",
        "Checking Contracts, Payment Methods, Internet Service, etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WrEgVSWdRA8T"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    cat_cols = ['Gender', 'SeniorCitizen', 'Partner', 'Dependents',\n",
        "                'PhoneService', 'InternetService', 'Contract', 'PaymentMethod']\n",
        "\n",
        "    # Filter cols that actually exist\n",
        "    cat_cols = [c for c in cat_cols if c in df.columns]\n",
        "\n",
        "    fig, axes = plt.subplots(4, 2, figsize=(16, 20))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    for i, col in enumerate(cat_cols):\n",
        "        if i < len(axes):\n",
        "            sns.countplot(data=df, x=col, hue='Churn', ax=axes[i], palette='viridis')\n",
        "            axes[i].set_title(f'Churn Rate by {col}')\n",
        "            axes[i].tick_params(axis='x', rotation=15)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/3_categorical_features.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hm2lqP5URA8U"
      },
      "source": [
        "### 7. Correlation Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tJhS2v2GRA8U"
      },
      "outputs": [],
      "source": [
        "if file_path:\n",
        "    # Convert Churn to numeric for correlation\n",
        "    df_corr = df.copy()\n",
        "    df_corr['Churn'] = df_corr['Churn'].apply(lambda x: 1 if x == 'Yes' else 0)\n",
        "\n",
        "    # Select only numeric columns\n",
        "    numeric_df = df_corr.select_dtypes(include=[np.number])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(numeric_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
        "    plt.title('Correlation Heatmap')\n",
        "    plt.savefig(f\"{OUTPUT_DIR}/4_correlation_heatmap.png\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wv9J2wnsRXc8"
      },
      "source": [
        "**Task 02**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eY0wptNDc9gV"
      },
      "source": [
        "### 1. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJK2__TTRVDK",
        "outputId": "7965319e-119c-4eae-c8d0-ee471ef81faa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output Directory Created: task02_results/\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "\n",
        "# Scikit-Learn Modules\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import (classification_report, confusion_matrix, accuracy_score, roc_curve, auc, f1_score)\n",
        "\n",
        "# --- 1. SETUP & CONFIGURATION ---\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Visualization settings\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "plt.rcParams['font.size'] = 12\n",
        "\n",
        "# Create output folder\n",
        "OUTPUT_DIRE = \"task02_results\"\n",
        "os.makedirs(OUTPUT_DIRE, exist_ok=True)\n",
        "print(f\"Output Directory Created: {OUTPUT_DIRE}/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHFsN_sicVuQ"
      },
      "source": [
        "### 2. Data loading & Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "idGsIs0ucwg6",
        "outputId": "6fdebadb-3197-4793-8522-d257efa6c0dc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 1] Loading Dataset...\n",
            " -> Loaded local file.\n",
            " -> Data Shape after cleaning: (7043, 20)\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[Step 2] Loading Dataset...\")\n",
        "\n",
        "filename = '/content/drive/MyDrive/cw/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "if os.path.exists(filename):\n",
        "    df = pd.read_csv(filename)\n",
        "    print(\" -> Loaded local file.\")\n",
        "else:\n",
        "    print(\"ERROR: CSV file not found! Please upload your dataset to the Colab Files tab on the left.\")\n",
        "\n",
        "# Data Cleaning\n",
        "# Fix TotalCharges: It has spaces \" \" for new customers. Coerce creates NaNs, we fill with 0.\n",
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)\n",
        "\n",
        "# Drop customerID (irrelevant unique identifier)\n",
        "if 'customerID' in df.columns:\n",
        "    df = df.drop(['customerID'], axis=1)\n",
        "\n",
        "# Encode Target: Yes -> 1, No -> 0\n",
        "df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "\n",
        "print(f\" -> Data Shape after cleaning: {df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VckYOMBfdLp7"
      },
      "source": [
        "### 3. Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcad96ysdcRX",
        "outputId": "7a270ed0-aafc-4fa4-85b5-a23df504a736"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 2] Preprocessing & Feature Engineering...\n",
            " -> Data successfully split and scaled.\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[Step 3] Preprocessing & Feature Engineering...\")\n",
        "\n",
        "# One-Hot Encoding\n",
        "# We use drop_first=True to avoid multicollinearity (dummy variable trap)\n",
        "df_encoded = pd.get_dummies(df, drop_first=True)\n",
        "\n",
        "# Define Features (X) and Target (y)\n",
        "X = df_encoded.drop('Churn', axis=1)\n",
        "y = df_encoded['Churn']\n",
        "\n",
        "# Stratified Split\n",
        "# 'stratify=y' ensures the % of churners is the same in Train and Test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=RANDOM_SEED, stratify=y\n",
        ")\n",
        "\n",
        "# Scaling\n",
        "# Neural Networks perform significantly better with scaled data (Mean=0, Std=1)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(\" -> Data successfully split and scaled.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdTPvTunden5"
      },
      "source": [
        "### 4. Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiAihSO7dmoG",
        "outputId": "fceeaeaf-9467-423b-b63d-79edb474e4d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 3] Training Decision Tree...\n",
            " -> Best Tree Params: {'class_weight': 'balanced', 'criterion': 'entropy', 'max_depth': 5, 'min_samples_split': 2}\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[Step 4] Training Decision Tree...\")\n",
        "\n",
        "# Parameter Grid\n",
        "# Included 'class_weight' to handle imbalance automatically\n",
        "dt_params = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [5, 10, 15, None],\n",
        "    'min_samples_split': [2, 10, 20],\n",
        "    'class_weight': [None, 'balanced']\n",
        "}\n",
        "\n",
        "# Grid Search\n",
        "dt_grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=RANDOM_SEED),\n",
        "    dt_params,\n",
        "    cv=5,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "dt_grid.fit(X_train_scaled, y_train)\n",
        "best_dt = dt_grid.best_estimator_\n",
        "\n",
        "print(f\" -> Best Tree Params: {dt_grid.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0Z_N5xRdvr8"
      },
      "source": [
        "### 5. Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h14JPhV1d4j3",
        "outputId": "bf63b36b-a3ce-44d1-ac6f-211a2bbabb5b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Step 4] Training Neural Network...\n",
            " -> Tuning Hyperparameters (this takes a moment)...\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n[Step 5] Training Neural Network...\")\n",
        "print(\" -> Tuning Hyperparameters (this takes a moment)...\")\n",
        "\n",
        "# Parameter Grid\n",
        "nn_params = {\n",
        "    'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'alpha': [0.0001, 0.01], # L2 regularization\n",
        "    'learning_rate_init': [0.001, 0.01]\n",
        "}\n",
        "\n",
        "# MLP Classifier\n",
        "mlp = MLPClassifier(max_iter=500, random_state=RANDOM_SEED)\n",
        "\n",
        "# Grid Search\n",
        "nn_grid = GridSearchCV(\n",
        "    mlp,\n",
        "    nn_params,\n",
        "    cv=3,\n",
        "    scoring='f1',\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "nn_grid.fit(X_train_scaled, y_train)\n",
        "best_nn = nn_grid.best_estimator_\n",
        "\n",
        "print(f\" -> Best Neural Net Params: {nn_grid.best_params_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H6OINRE0d-fx"
      },
      "source": [
        "### 6. Generating Plots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJBQVj_EeHI2"
      },
      "outputs": [],
      "source": [
        "print(\"\\n[Step 6] Evaluation & Generating Plots...\")\n",
        "\n",
        "# Function to plot Confusion Matrix\n",
        "def plot_cm(model, X_test, y_test, title, filename):\n",
        "    y_pred = model.predict(X_test)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
        "    plt.title(title)\n",
        "    plt.ylabel('Actual Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(OUTPUT_DIRE, filename), dpi=300)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WS6V3XcBU_kR"
      },
      "source": [
        "# Task 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W9uifLUWVBhy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from scipy.stats import ks_2samp\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (10, 6)\n",
        "\n",
        "def load_data():\n",
        "    # Re-load data to ensure we have the raw columns for Ethics check\n",
        "    filename = '/content/drive/MyDrive/cw/WA_Fn-UseC_-Telco-Customer-Churn.csv'\n",
        "    df = pd.read_csv(filename)\n",
        "\n",
        "    # Basic cleanup\n",
        "    df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce').fillna(0)\n",
        "    df['Churn'] = df['Churn'].map({'Yes': 1, 'No': 0})\n",
        "    return df\n",
        "\n",
        "# ==========================================\n",
        "# PART 1: AI ETHICS (Bias Detection)\n",
        "# ==========================================\n",
        "def check_fairness(df):\n",
        "    print(\"\\n--- 1. AI Ethics: Fairness & Bias Analysis ---\")\n",
        "\n",
        "    # We check if the Churn Rate is significantly different for protected groups\n",
        "    protected_groups = ['gender', 'SeniorCitizen']\n",
        "\n",
        "    for group in protected_groups:\n",
        "        print(f\"\\nAnalyzing Bias in: {group}\")\n",
        "\n",
        "        # Calculate Churn Rate per group\n",
        "        bias_check = df.groupby(group)['Churn'].mean().reset_index()\n",
        "        bias_check.columns = [group, 'Churn_Rate']\n",
        "\n",
        "        print(bias_check)\n",
        "\n",
        "        # Visualization\n",
        "        plt.figure(figsize=(6, 4))\n",
        "        sns.barplot(x=group, y='Churn_Rate', data=bias_check, palette='magma')\n",
        "        plt.title(f'Fairness Check: Churn Rate by {group}')\n",
        "        plt.ylabel('Churn Proportion')\n",
        "        plt.ylim(0, 1)\n",
        "\n",
        "        # Add values\n",
        "        for index, row in bias_check.iterrows():\n",
        "            plt.text(index, row.Churn_Rate + 0.02, f'{row.Churn_Rate:.2f}', ha='center')\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "        # Ethics Interpretation\n",
        "        rates = bias_check['Churn_Rate'].values\n",
        "        diff = abs(rates[0] - rates[1])\n",
        "        if diff > 0.1:\n",
        "            print(f\"POTENTIAL BIAS DETECTED: The difference in churn predictions for {group} is large ({diff:.2f}).\")\n",
        "            print(\"Strategy: We must ensure the model isn't using this feature to discriminate unlawfully.\")\n",
        "        else:\n",
        "            print(f\"FAIRNESS CHECK PASSED: The difference is small ({diff:.2f}).\")\n",
        "\n",
        "# ==========================================\n",
        "# PART 2: POST-DEPLOYMENT STRATEGY (Drift)\n",
        "# ==========================================\n",
        "def simulate_deployment_monitoring(df):\n",
        "    print(\"\\n--- 2. Post-Deployment: Data Drift Monitoring ---\")\n",
        "    print(\"Simulating next month's data to check if model needs retraining...\")\n",
        "\n",
        "    # Simulate \"New Data\" (e.g., Next month, customers are slightly different)\n",
        "    # We artificially increase MonthlyCharges to simulate inflation/price hikes\n",
        "    current_data = df['MonthlyCharges']\n",
        "    new_data = df['MonthlyCharges'] * 1.15 + np.random.normal(0, 5, len(df))\n",
        "\n",
        "    # Visualization of Drift\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    sns.kdeplot(current_data, shade=True, label='Training Data (Baseline)', color='blue')\n",
        "    sns.kdeplot(new_data, shade=True, label='New Data (Live)', color='red')\n",
        "    plt.title('Data Drift Detection: Monthly Charges')\n",
        "    plt.xlabel('Monthly Charges')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # Statistical Test (Kolmogorov-Smirnov Test)\n",
        "    stat, p_value = ks_2samp(current_data, new_data)\n",
        "    print(f\"Drift Test (KS-Test) P-Value: {p_value:.5f}\")\n",
        "\n",
        "    if p_value < 0.05:\n",
        "        print(\"DATA DRIFT DETECTED!\")\n",
        "        print(\"Strategy: The data distribution has changed significantly.\")\n",
        "        print(\"Action: Trigger automatic retraining pipeline.\")\n",
        "    else:\n",
        "        print(\"No Drift Detected. Model is stable.\")\n",
        "\n",
        "# --- EXECUTION ---\n",
        "if __name__ == \"__main__\":\n",
        "    df = load_data()\n",
        "    check_fairness(df)\n",
        "    simulate_deployment_monitoring(df)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
